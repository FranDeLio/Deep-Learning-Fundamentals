{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class DatasetClass(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Read data\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "\n",
    "# train-test split for model evaluation\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    "\n",
    "\n",
    "# Standardizing data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Convert to 2D PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Instantiate your dataset\n",
    "train_dataset = DatasetClass(X_train, y_train)\n",
    "test_dataset = DatasetClass(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class DropoutDeepRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(8, 60)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.layer2 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.layer3 = nn.Linear(60, 30)\n",
    "        self.act3 = nn.LeakyReLU()\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.output = nn.Linear(30, 1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "class StandardDeepRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(8, 60)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.layer2 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.layer3 = nn.Linear(60, 30)\n",
    "        self.act3 = nn.LeakyReLU()\n",
    "        self.output = nn.Linear(30, 1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "standard_model = StandardDeepRegressor()\n",
    "dropout_model = DropoutDeepRegressor()\n",
    "# loss function and optimizer\n",
    "loss_fn = nn.MSELoss()  # mean square error\n",
    "\n",
    "n_epochs = 500   # number of epochs to run\n",
    "batch_size = 10  # size of each batch\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Hold the best model\n",
    "  # init to infinity\n",
    "best_weights = None\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    batch_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "    mse = np.mean(batch_loss)\n",
    "    return mse\n",
    "\n",
    "def train_model(model, train_loader, test_loader, loss_fn, n_epochs):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001) #weight_decay=0.1\n",
    "    optimizer_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.666)\n",
    "    best_mse = np.inf \n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            batch_loss = []\n",
    "            with tqdm(total=len(train_loader), unit=\"batch\") as bar:\n",
    "                bar.set_description(f\"Epoch {epoch} (LR={optimizer.param_groups[0]['lr']})\")\n",
    "                \n",
    "                for X_batch, y_batch in train_loader:\n",
    "                    # Forward pass\n",
    "                    y_pred = model.forward(X_batch)\n",
    "                    loss = loss_fn(y_pred, y_batch)\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Update weights\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    batch_loss.append(loss.item())\n",
    "                    bar.set_postfix(mse=np.mean(batch_loss))\n",
    "                    bar.update(1)\n",
    "\n",
    "            # Evaluate accuracy at the end of each epoch\n",
    "            mse = evaluate(model, train_loader, loss_fn)\n",
    "            history.append(mse)\n",
    "            \n",
    "            # Take step toward reducing learning rate\n",
    "            optimizer_scheduler.step()\n",
    "\n",
    "            # Check if the current model is the best\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    test_mse = evaluate(model, test_loader, loss_fn)\n",
    "\n",
    "    return model, best_mse, best_weights, test_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 (LR=0.0001): 100%|██████████| 1445/1445 [00:08<00:00, 177.40batch/s, mse=1.66]\n",
      "Epoch 1 (LR=0.0001): 100%|██████████| 1445/1445 [00:08<00:00, 175.49batch/s, mse=0.655]\n",
      "Epoch 2 (LR=0.0001): 100%|██████████| 1445/1445 [00:08<00:00, 161.50batch/s, mse=0.532]\n",
      "Epoch 3 (LR=0.0001): 100%|██████████| 1445/1445 [00:09<00:00, 159.28batch/s, mse=0.488]\n",
      "Epoch 4 (LR=0.0001): 100%|██████████| 1445/1445 [00:07<00:00, 183.89batch/s, mse=0.463]\n",
      "Epoch 5 (LR=0.0001): 100%|██████████| 1445/1445 [00:08<00:00, 177.13batch/s, mse=0.441]\n",
      "Epoch 6 (LR=0.0001): 100%|██████████| 1445/1445 [00:07<00:00, 183.35batch/s, mse=0.423]\n",
      "Epoch 7 (LR=0.0001): 100%|██████████| 1445/1445 [00:08<00:00, 176.97batch/s, mse=0.407]\n",
      "Epoch 8 (LR=0.0001): 100%|██████████| 1445/1445 [00:08<00:00, 160.85batch/s, mse=0.395]\n",
      "Epoch 9 (LR=0.0001): 100%|██████████| 1445/1445 [00:08<00:00, 169.01batch/s, mse=0.385]\n",
      "Epoch 10 (LR=0.0001): 100%|██████████| 1445/1445 [00:08<00:00, 162.79batch/s, mse=0.377]\n",
      "Epoch 11 (LR=0.0001): 100%|██████████| 1445/1445 [00:09<00:00, 155.60batch/s, mse=0.37] \n",
      "Epoch 12 (LR=0.0001): 100%|██████████| 1445/1445 [00:07<00:00, 189.91batch/s, mse=0.364]\n",
      "Epoch 13 (LR=0.0001): 100%|██████████| 1445/1445 [00:09<00:00, 150.54batch/s, mse=0.359]\n",
      "Epoch 14 (LR=0.0001): 100%|██████████| 1445/1445 [00:08<00:00, 179.24batch/s, mse=0.354]\n",
      "Epoch 15 (LR=0.0001): 100%|██████████| 1445/1445 [00:09<00:00, 146.83batch/s, mse=0.35] \n",
      "Epoch 16 (LR=0.0001):  62%|██████▏   | 898/1445 [00:05<00:03, 157.41batch/s, mse=0.347]"
     ]
    }
   ],
   "source": [
    "standard_model, best_mse, best_weights, test_mse = train_model(standard_model, train_loader, test_loader, loss_fn, n_epochs)\n",
    "\n",
    "# Restore the best model\n",
    "standard_model.load_state_dict(best_weights)\n",
    "print(\"Best MSE: %.2f\" % best_mse)\n",
    "print(\"Best RMSE: %.2f\" % np.sqrt(best_mse))\n",
    "print(\"Best MSE: %.2f\" % test_mse)\n",
    "plt.plot(history)\n",
    "plt.show()\n",
    "\n",
    "# Test inference with 5 samples\n",
    "standard_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        X_sample = X_test_raw[i: i+1]\n",
    "        X_sample = scaler.transform(X_sample)\n",
    "        X_sample = torch.tensor(X_sample, dtype=torch.float32)\n",
    "        y_pred = standard_model(X_sample)\n",
    "        print(f\"{X_test_raw[i]} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, best_mse, best_weights, test_mse = train_model(dropout_model, train_loader, test_loader, loss_fn, n_epochs)\n",
    "\n",
    "# Restore the best model\n",
    "model.load_state_dict(best_weights)\n",
    "print(\"Best MSE: %.2f\" % best_mse)\n",
    "print(\"Best RMSE: %.2f\" % np.sqrt(best_mse))\n",
    "print(\"Best MSE: %.2f\" % test_mse)\n",
    "plt.plot(history)\n",
    "plt.show()\n",
    "\n",
    "# Test inference with 5 samples\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        X_sample = X_test_raw[i: i+1]\n",
    "        X_sample = scaler.transform(X_sample)\n",
    "        X_sample = torch.tensor(X_sample, dtype=torch.float32)\n",
    "        y_pred = model(X_sample)\n",
    "        print(f\"{X_test_raw[i]} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
